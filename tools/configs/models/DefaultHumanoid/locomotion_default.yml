seed: 0

env:
  name: "default_humanoid"
  num_joints: 12
  ctrl_dt: 0.02  
  sim_dt: 0.002
  episode_length: 1000
  action_repeat: 1
  action_scale: 0.5
  history_len: 1
  soft_joint_pos_limit_factor: 0.95 #Soft bound to discourage maxing out joint limits.
  noise_config:
    level: 0.0  # Set to 0.0 to disable noise.
    scales:
      hip_pos: 0.03  # rad
      kfe_pos: 0.05
      ffe_pos: 0.08
      faa_pos: 0.03
      joint_vel: 1.5  # rad/s
      gravity: 0.05
      linvel: 0.1
      gyro: 0.2  # angvel.
    
  d_randomization: False #Doesn't work !
  
  reward_config:
    scales: 
      # Tracking related rewards.
      tracking_lin_vel: 1.0
      tracking_ang_vel: 0.5
      # Base related rewards.
      lin_vel_z: 0.0
      ang_vel_xy: -0.15
      orientation: -1.0
      base_height: 0.0
      # Energy related rewards.
      torques: -2.5e-5
      action_rate: -0.01
      energy: 0.0
      # Feet related rewards.
      feet_clearance: 0.0
      feet_air_time: 2.0
      feet_slip: -0.25
      feet_height: 0.0
      feet_phase: 1.0
      # Other rewards.
      stand_still: 0.0
      alive: 0.0
      termination: -1.0
      # Pose related rewards.
      joint_deviation_knee: -0.1
      joint_deviation_hip: -0.25
      dof_pos_limits: -1.0
      pose: -1.0
    
    tracking_sigma: 0.5 # Standard deviation for tracking velocity rewards
    max_foot_height: 0.2
    base_height_target: 0.9 
  
  push_config:
    enable: False
    interval_range: [5.0, 10.0]
    magnitude_range: [0.1, 2.0]
  
  lin_vel_x: [-1.0, 1.0]
  lin_vel_y: [-1.0, 1.0]
  ang_vel_yaw: [-1.0, 1.0]

logger:
  tensorboard: True
  wandb: False
  project_name: "default_humanoid"

brax_ppo_agent:
  num_timesteps: 150_000_000
  num_evals: 15
  clipping_epsilon: 0.2
  reward_scaling: 1.0
  episode_length: 1000
  normalize_observations: True
  action_repeat: 1
  unroll_length: 20
  num_minibatches: 32
  num_updates_per_batch: 4
  discounting: 0.97
  learning_rate: 0.0003
  entropy_cost: 0.005
  num_envs: 8192
  batch_size: 256
  max_grad_norm: 1.0
  network_factory:
    policy_hidden_layer_sizes: [512, 256, 128]
    value_hidden_layer_sizes: [512, 256, 128]
    policy_obs_key: "state"
    value_obs_key: "privileged_state"
  num_resets_per_eval : 1
