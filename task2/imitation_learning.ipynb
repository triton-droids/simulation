{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Code / Packages\n",
    "If you are running the code locally install ..... \n",
    "\n",
    "If you are using Google Colab make sure to run the cell below to install all dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.spaces import Box\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import mujoco\n",
    "import mplib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Creating a Standardized Environment with Gymnasium\n",
    "In this part, you will implement the `TrajEnv` class to wrap a MuJoCo simulation in a Gymnasium interface, providing a consistent way to reset the environment, step through actions, and collect observations. \n",
    "\n",
    "Your task is to decide which observations are useful as states in a behavior cloning setting. You will also implement the `is_grasped` property to define when the robot has successfully grasped the object and the `terminated` property to specify when the cube is in the bin. The `reset_model` method should randomize initial arm and object positions to encourage generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.spaces import Box\n",
    "import numpy as np\n",
    "import mujoco\n",
    "\n",
    "\n",
    "class TrajEnv(MujocoEnv):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"depth_array\"]}\n",
    "\n",
    "    def __init__(self, xml_file: str, frame_skip: int = 5, **kwargs):\n",
    "        # TODO: Define the observation space and store any useful state variables.\n",
    "        observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(000,), dtype=np.float32  # TODO: set correct shape\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            xml_file,\n",
    "            frame_skip,\n",
    "            observation_space=observation_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.panda_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'panda_arm')\n",
    "        \n",
    "\n",
    "    \n",
    "    @property\n",
    "    def is_grasped(self):\n",
    "        # TODO: Implement a condition to determine if the object\n",
    "        # is grasped. Could use distance between gripper and object, contact forces, etc.\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def terminated(self):\n",
    "        # TODO: Define the termination condition for an episode.\n",
    "        ...\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # TODO: Return the observations selected for the environment state\n",
    "        # Example: joint positions, velocities, end-effector pose, object positions\n",
    "        ...\n",
    "\n",
    "    def step(self, action) -> tuple[np.ndarray, bool, bool]:\n",
    "        # DO NOT MODIFY THIS FUNCTION\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        obs = self._get_obs()\n",
    "        terminated = self.terminated\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return obs, None, terminated, False, None\n",
    "\n",
    "    def reset_model(self):\n",
    "        self.default_qpos = np.array(\n",
    "                self.model.keyframe('home').qpos,\n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "        qpos = self.default_qpos.copy()\n",
    "        qvel = np.zeros(self.model.nv)\n",
    "\n",
    "        # TODO: Randomize initial positions for the arm (9 joints) and objects\n",
    "        # to create diverse starting conditions. Ensure valid states and objects are not overlapping.\n",
    "        ...\n",
    "\n",
    "        self.set_state(qpos, qvel)\n",
    "        return self._get_obs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what is happening in the environment, you can call `env.render()` to visualize it.  \n",
    "Since we set `render_mode=\"rgb_array\"`, `env.render()` will return an RGB array that can be displayed.  \n",
    "\n",
    "> **Note:** If the default view of the environment is not ideal, you may need to adjust the camera.  \n",
    "> You can do this by defining a camera in your Mujoco XML file and then passing its name to the `MujocoEnv` constructor via the `camera_name` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = TrajEnv(\"\", 20, render_mode='rgb_array')\n",
    "image = env.render() \n",
    "image = image\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these core functionalities we can build the basic interaction loop that consists of a reset followed by steps until terminated or truncated is True. We can also record a video as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "images = [env.render()]\n",
    "# keep taking steps until either we terminated or truncate\n",
    "while not done:\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    images.append(env.render())\n",
    "    done = terminated or truncated\n",
    "\n",
    "# save a video\n",
    "from mani_skill.utils.visualization import images_to_video\n",
    "images_to_video(images, output_dir=\"videos\", video_name=\"example\", fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"./videos/example.mp4\", embed=True, width=640) # Watch our replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collect your Imitation Learning Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Setup =====\n",
    "env = TrajEnv(\n",
    "    xml_file=\"/home/anthony-roumi/Desktop/sim_onboarding/descriptions/DropCubeInBinPandaEnv.xml\", \n",
    "    frame_skip=20,\n",
    "    \n",
    ")\n",
    "#TODO: Define the link and joint names for the planner. How can we do this using MuJoCo?\n",
    "link_names = []\n",
    "joint_names = []\n",
    "\n",
    "planner = mplib.Planner(\n",
    "    urdf='assets/robots/panda/panda_v2.urdf',\n",
    "    srdf='assets/robots/panda/panda_v2.srdf',\n",
    "    user_link_names=link_names,\n",
    "    user_joint_names=joint_names,\n",
    "    move_group=\"panda_hand_tcp\",\n",
    "    joint_vel_limits=np.ones(7) * 0.8,\n",
    "    joint_acc_limits=np.ones(7) * 0.8,\n",
    ")\n",
    "\n",
    "# Helper: execute planned motion and record data\n",
    "def move_to_pose(pose, gripper, episode_data):\n",
    "    result = planner.plan_screw(\n",
    "            pose,\n",
    "            env.data.qpos[:9],\n",
    "            time_step=env.dt,\n",
    "        )\n",
    "    if 'Success' in result:\n",
    "        for pos in result[\"position\"]:\n",
    "            action = np.concatenate([pos, [gripper]])\n",
    "            obs = env._get_obs()\n",
    "            env.step(action)\n",
    "            episode_data.append((obs, action))\n",
    "    else:\n",
    "        print(f\"Planner failed: {result}\")\n",
    "        return None  # Signal failure\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = []\n",
    "num_episodes = 10_000\n",
    "success_count = 0\n",
    "ep = 0\n",
    "\n",
    "while ep < num_episodes:\n",
    "    env.reset()\n",
    "    episode_data = []\n",
    "\n",
    "    # IDs for objects\n",
    "    panda_id = env.panda_id\n",
    "    cube_body_id = env.cube_body_id\n",
    "    bin_body_id = env.bin_body_id\n",
    "    tcp_body_id = env.tcp_body_id\n",
    "\n",
    "    # this sets the planner object up such that you can plan with poses in the world frame.\n",
    "    planner.set_base_pose(np.concatenate([env.data.xpos[panda_id], env.data.xquat[panda_id]]))\n",
    "\n",
    "    # Get positions/quats\n",
    "    cube_pose = env.data.xpos[cube_body_id]\n",
    "    cube_quat = env.data.xquat[cube_body_id]\n",
    "    bin_pose = env.data.xpos[bin_body_id]\n",
    "    bin_quat = env.data.xquat[bin_body_id]\n",
    "    tcp_quat = env.data.xquat[tcp_body_id]\n",
    "\n",
    "    try:\n",
    "        # ==== Motion Sequence ====\n",
    "        # 1. Move above cube\n",
    "        r_tcp_rel = R.from_quat(tcp_quat).inv() * R.from_quat(cube_quat)\n",
    "        tcp_quat_new = (R.from_quat(cube_quat) * r_tcp_rel).as_quat()\n",
    "        above_cube = np.concatenate([cube_pose + np.array([0, 0, 0.1]), tcp_quat_new])\n",
    "        last_pos = move_to_pose(above_cube, 255, episode_data)\n",
    "        if last_pos is None: \n",
    "            raise RuntimeError(\"Skipping episode due to planner failure at above_cube\")\n",
    "\n",
    "        # 2. Move down to cube\n",
    "        # cube_pose = cube_pose + np.array([0, 0, 0.04])\n",
    "        to_cube = np.concatenate([cube_pose, tcp_quat_new])\n",
    "        last_pos = move_to_pose(to_cube, 255, episode_data)\n",
    "        if last_pos is None: \n",
    "            raise RuntimeError(\"Skipping episode due to planner failure at to_cube\")\n",
    "\n",
    "        # 3. Close gripper\n",
    "        for step in range(50):\n",
    "            action = np.concatenate([last_pos, [0]])\n",
    "            obs = env._get_obs()\n",
    "            env.step(action)\n",
    "            episode_data.append((obs, action))\n",
    "\n",
    "        # 4. Move above bin\n",
    "        r_tcp_rel = R.from_quat(tcp_quat).inv() * R.from_quat(bin_quat)\n",
    "        tcp_quat_new = (R.from_quat(bin_quat) * r_tcp_rel).as_quat()\n",
    "        above_bin = np.concatenate([bin_pose + np.array([0, 0, 0.2]), tcp_quat_new])\n",
    "        last_pos = move_to_pose(above_bin, 0, episode_data)\n",
    "        if last_pos is None: \n",
    "            raise RuntimeError(\"Skipping episode due to planner failure at above_bin\")\n",
    "\n",
    "        # 5. Move down to bin\n",
    "        to_bin = np.concatenate([bin_pose + np.array([0, 0, 0.1]), tcp_quat_new])\n",
    "        last_pos = move_to_pose(to_bin, 0, episode_data)\n",
    "        if last_pos is None: \n",
    "            raise RuntimeError(\"Skipping episode due to planner failure at to_bin\")\n",
    "\n",
    "        # 6. Open gripper\n",
    "        for step in range(25):\n",
    "            action = np.concatenate([last_pos, [255]])\n",
    "            obs = env._get_obs()\n",
    "            env.step(action)\n",
    "            episode_data.append((obs, action))\n",
    "\n",
    "        if env.terminated:\n",
    "            trajectories.append(episode_data)\n",
    "            print(f\"Episode {ep} completed successfully.\")\n",
    "            success_count += 1\n",
    "        ep += 1  # Only increment episode count if successful\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        print(\"Resetting environment and skipping this trajectory.\")\n",
    "        print('='*100)\n",
    "        continue\n",
    "    \n",
    "# ===== Save dataset =====\n",
    "with open(\"pick_place_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trajectories, f)\n",
    "\n",
    "print(f\"Success rate: {success_count / num_episodes}\")\n",
    "print(f\"Total trajectories: {len(trajectories)}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #This should return observation idx and action idx\n",
    "        pass\n",
    "\n",
    "\n",
    "dataset_file = \"pick_place_dataset.pkl\"\n",
    "with open(dataset_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "dataset = TrajectoryDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define Policy Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "actuator_ctrl_range = torch.tensor([[-2.8973e+00,  2.8973e+00],\n",
    "       [-1.7628e+00,  1.7628e+00],\n",
    "       [-2.8973e+00,  2.8973e+00],\n",
    "       [-3.0718e+00, -6.9800e-02],\n",
    "       [-2.8973e+00,  2.8973e+00],\n",
    "       [-1.7500e-02,  3.7525e+00],\n",
    "       [-2.8973e+00,  2.8973e+00],\n",
    "       [ 0.0000e+00,  2.5500e+02]])\n",
    "\n",
    "class GaussianActor(nn.Module):\n",
    "    def __init__(self, sample_obs, sample_act, ctrl_ranges=actuator_ctrl_range):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = sample_obs.shape[-1]\n",
    "        self.output_dim = sample_act.shape[-1]\n",
    "\n",
    "        # store actuator ranges\n",
    "        self.ctrl_low = torch.tensor(ctrl_ranges[:, 0], dtype=torch.float32)\n",
    "        self.ctrl_high = torch.tensor(ctrl_ranges[:, 1], dtype=torch.float32)\n",
    "        \n",
    "        # shared backbone\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.LayerNorm(256),   # helps stability\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # mean head\n",
    "        self.mean_head = nn.Linear(256, self.output_dim)\n",
    "        # log std head (trainable variance per action dimension)\n",
    "        self.log_std_head = nn.Linear(256, self.output_dim)\n",
    "\n",
    "        # initialize\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        nn.init.xavier_uniform_(self.mean_head.weight)\n",
    "        nn.init.zeros_(self.mean_head.bias)\n",
    "        nn.init.xavier_uniform_(self.log_std_head.weight)\n",
    "        nn.init.zeros_(self.log_std_head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "\n",
    "        mean_raw = self.mean_head(h)\n",
    "        log_std = self.log_std_head(h).clamp(-5, 2)  # std âˆˆ [0.05, 1.65]\n",
    "\n",
    "\n",
    "        # squash mean into [-1,1]\n",
    "        mean_squashed = torch.tanh(mean_raw)\n",
    "\n",
    "        # affine transform to actuator ranges\n",
    "        mean = self.ctrl_low.to(mean_squashed.device) + 0.5 * (mean_squashed + 1.0) * (\n",
    "            self.ctrl_high - self.ctrl_low\n",
    "        ).to(mean_squashed.device)\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, x):\n",
    "        mean, log_std = self.forward(x)\n",
    "        std = log_std.exp()\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.rsample()   # reparameterized sample\n",
    "        return action, dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(eval_env, actor, num_episodes=5):\n",
    "    success = 0\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = eval_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        while not (truncated):\n",
    "            with torch.no_grad():\n",
    "                action, _ = actor(torch.from_numpy(obs).to(device).float())\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action.cpu().numpy())\n",
    "\n",
    "        if terminated:\n",
    "            success += 1\n",
    "        \n",
    "        print(f\"Episode {ep+1}: Success={terminated}\")\n",
    "    return success / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(actor, obs, expert_actions):\n",
    "    mean, log_std = actor(obs)\n",
    "    std = log_std.exp()\n",
    "\n",
    "\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    log_prob = dist.log_prob(expert_actions).sum(-1)   # sum over action dims\n",
    "    loss = -log_prob.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 3e-5\n",
    "best_success_rate = 0.0\n",
    "eval_interval = 5\n",
    "best_loss = float('inf')\n",
    "\n",
    "optimizer = torch.optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    actor.train()\n",
    "    total_loss = 0\n",
    "        \n",
    "    for obs, actions in dataloader:\n",
    "        obs, actions = obs.to(device).float(), actions.to(device).float()\n",
    "        mean, log_std = actor(obs)   # forward pass\n",
    "\n",
    "        \n",
    "        loss = nll_loss(actor, obs, actions)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    if (epoch + 1) % eval_interval == 0:\n",
    "        success_rate = eval_policy(eval_env, actor)\n",
    "        print(f'Evaluation at epoch {epoch+1}, Success Rate: {success_rate:.2%}')\n",
    "        \n",
    "        if success_rate > best_success_rate:\n",
    "            best_success_rate = success_rate\n",
    "            torch.save(actor.state_dict(), 'best_actor_success.pth')\n",
    "            print(f'New best model saved with success rate:')\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(actor.state_dict(), 'best_actor_loss.pth')\n",
    "            print(f'New best model saved with best loss:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate in Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load('best_actor_loss.pth'))\n",
    "eval_policy(eval_env, actor, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maniskill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
