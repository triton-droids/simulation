{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Code / Packages\n",
    "If you are running the code locally follow the instructions [here](../docs/setup.md).\n",
    "\n",
    "If you are using Google Colab make sure to run the cell below to install all dependencies. You will also need to upload your xml file to the assets folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for the notebook\n",
    "!pip install gymnasium[mujoco]  # includes mujoco and mujoco-py dependencies\n",
    "!pip install numpy scipy matplotlib imageio\n",
    "!pip install mplib  \n",
    "\n",
    "#Download the assets folder\n",
    "!git init temp-repo\n",
    "%cd temp-repo\n",
    "!git remote add -f origin https://github.com/triton-droids/simulation.git\n",
    "!git config core.sparseCheckout true\n",
    "!echo \"assets/*\" >> .git/info/sparse-checkout\n",
    "!git pull origin onboarding\n",
    "%cd ..\n",
    "!mv temp-repo/assets ./assets\n",
    "!rm -rf temp-repo\n",
    "\n",
    "# Configure MuJoCo for Colab\n",
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'egl'  # enables headless rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gymnasium\n",
    "No problems are assigned here, but understanding the code APIs/interfaces and functionalities are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymnasium Basics\n",
    "The Gymnasium package is a standardized API for creating and interacting with simulation environments. \n",
    "\n",
    "<img src=\"../assets/media/gym_loop.png\" alt=\"agent-environment loop\" width=\"500\"/>\n",
    "\n",
    "Gymnasium defines the agent-environment loop â€” a simple interface where an agent takes actions and receives observations, rewards, and termination signals from an environment. This loop forms the foundation for reinforcement learning (RL) and allows us to easily manipulate the simulator.\n",
    "\n",
    "**Why We Use Gymnasium**\n",
    "\n",
    "Gymnasium acts as a common interface layer between learning algorithms and simulation environments. It provides:\n",
    "- A uniform API (reset(), step(), observation, reward, done, info) \n",
    "- Integration with RL algorithm libraries (e.g., Stable-Baselines3, CleanRL).\n",
    "- Rendering and debugging utilities for visualization and evaluation.\n",
    "\n",
    "\n",
    "In this task, you will use Gymnasium as the primary API to interface with the simulator to :\n",
    "- Collect trajectory data for imitation learning.\n",
    "- Evaluate and deploy trained policies.\n",
    "\n",
    "Read their documentation [here](https://gymnasium.farama.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Creating a Standardized Environment with Gymnasium\n",
    "In this part, you will implement the `TrajEnv` class to wrap a MuJoCo simulation in a Gymnasium interface.\n",
    "\n",
    "Your task is to decide which observations are useful as states in a behavior cloning setting. You will also implement the `is_grasped` property to define when the robot has successfully grasped the object and the `terminated` property to specify when the cube is in the bin. The `reset_model` method should randomize initial arm and object positions to encourage generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "import numpy as np\n",
    "import mujoco\n",
    "\n",
    "class TrajEnv(MujocoEnv):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"depth_array\"]}\n",
    "\n",
    "    def __init__(self, xml_file: str, frame_skip: int = 5, **kwargs):\n",
    "        # TODO: Define the observation space and store any useful state variables.\n",
    "        observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(???,), dtype=np.float32  # TODO: set correct shape\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            xml_file,\n",
    "            frame_skip,\n",
    "            observation_space=observation_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.panda_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'panda_arm')\n",
    "        \n",
    "\n",
    "    \n",
    "    @property\n",
    "    def is_grasped(self):\n",
    "        # TODO: Implement a condition to determine if the object\n",
    "        # is grasped. Could use distance between gripper and object, contact forces, etc.\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def terminated(self):\n",
    "        # TODO: Define the termination condition for an episode.\n",
    "        pass\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # TODO: Return the observations selected for the environment state\n",
    "        # Example: joint positions, velocities, end-effector pose, object positions\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> tuple[np.ndarray, bool, bool]:\n",
    "        # DO NOT MODIFY THIS FUNCTION\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        obs = self._get_obs()\n",
    "        terminated = self.terminated\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return obs, None, terminated, False, None\n",
    "\n",
    "    def reset_model(self):\n",
    "        self.default_qpos = np.array(\n",
    "                self.model.keyframe('home').qpos,\n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "        qpos = self.default_qpos.copy()\n",
    "        qvel = np.zeros(self.model.nv)\n",
    "\n",
    "        # TODO: Randomize initial positions for the arm (9 joints) and objects\n",
    "        # to create diverse starting conditions. Ensure valid states and objects are not overlapping.\n",
    "        \n",
    "\n",
    "        self.set_state(qpos, qvel)\n",
    "        return self._get_obs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what is happening in the environment, you can call `env.render()` to visualize it.  \n",
    "Since we set `render_mode=\"rgb_array\"`, `env.render()` will return an RGB array that can be displayed.  \n",
    "\n",
    "\n",
    "> **Note:** If the default view of the environment is not ideal, you may need to adjust the camera.  \n",
    "> You can do this by defining a camera in your Mujoco XML file and then passing its name as a kwarg to the `TrajEnv` constructor via the `camera_name` argument.\n",
    "\n",
    "\n",
    "If you are on colab you can render the environment like so. If you are running MuJoCo locally you can use `render_mode=\"human\"` to display viewing window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = TrajEnv(\"PUT ABSOLUTE PATH HERE\", 20, render_mode='rgb_array', camera_name=\"fixed\")\n",
    "env = TimeLimit(env, 100)\n",
    "env.reset()\n",
    "image = env.render() \n",
    "image = image\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these core functionalities we can build the basic interaction loop that consists of a reset followed by steps until terminated or truncated is True. We can also record a video as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "images = [env.render()]\n",
    "# keep taking steps until either we terminated or truncate\n",
    "while not done:\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    images.append(env.render())\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import imageio\n",
    "\n",
    "frames = images \n",
    "frames = [frame.astype(np.uint8) for frame in frames]   \n",
    "\n",
    "# Write video to file\n",
    "video_path = \"output_video.mp4\"\n",
    "with imageio.get_writer(video_path, fps=20) as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "\n",
    "# Display video in notebook\n",
    "Video(video_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try and control the robot to grasp the cube, pick it up, and drop it in the wooden bin. \n",
    "\n",
    "The following code sets up a motion planning library for you and saves some useful variables. \n",
    "- `planner`: The planner objects uses screw motion planning.\n",
    "- `panda_hand_tcp`: The panda hand tool center point \"link\", which gives the current pose of the link we are controlling via the motion planning library. This is the point in the middle between the two panda arm grippers.\n",
    "- `cube`: The cube object, which you can use to acccess its pose.\n",
    "- `bin`: The bin object.\n",
    "\n",
    "The motion planner setup will generate a sequence joint position actions for all the joints excluding the gripper. The gripper actions you can manually pick (can range from 0 to 255). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplib\n",
    "\n",
    "env = TrajEnv(\n",
    "    xml_file=\"PUT ABSOLUTE PATH HERE\", \n",
    "    frame_skip=20,\n",
    "    render_mode=\"human\"\n",
    "    \n",
    ")\n",
    "env.reset(seed=42)\n",
    "\n",
    "link_names = [\n",
    "    'panda_link0','panda_link1','panda_link2','panda_link3','panda_link4',\n",
    "    'panda_link5','panda_link6','panda_link7','panda_link8','panda_hand',\n",
    "    'panda_hand_tcp','panda_leftfinger','panda_rightfinger',\n",
    "    'panda_leftfinger_pad','panda_rightfinger_pad'\n",
    "]\n",
    "joint_names = [\n",
    "    'panda_joint1','panda_joint2','panda_joint3','panda_joint4',\n",
    "    'panda_joint5','panda_joint6','panda_joint7','panda_finger_joint1','panda_finger_joint2'\n",
    "]\n",
    "\n",
    "planner = mplib.Planner(\n",
    "    urdf='../assets/descriptions/panda/urdf/panda.urdf',\n",
    "    srdf='../assets/descriptions/panda/srdf/panda.srdf',\n",
    "    user_link_names=link_names,\n",
    "    user_joint_names=joint_names,\n",
    "    move_group=\"panda_hand_tcp\",\n",
    "    joint_vel_limits=np.ones(7) * 0.8,\n",
    "    joint_acc_limits=np.ones(7) * 0.8,\n",
    ")\n",
    "\n",
    "# this sets the planner object up such that you can plan with poses in the world frame, which is the default frame of all pose data\n",
    "# in our simulator\n",
    "planner.set_base_pose(np.concatenate([env.data.xpos[env.panda_id], env.data.xquat[env.panda_id]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes for this task**\n",
    "\n",
    "The goal is to use the motion planner to pick up the cube and drop it into the bin.\n",
    "\n",
    "- You will need to access object positions and orientations from the simulator (env.data.xpos, env.data.xquat, etc.).\n",
    "- The motion planner requires a target pose (7,) for the robot hand (position + quaternion) and the current joint configuration (qpos). Both are expressed in the world frame.\n",
    "- Plan a trajectory to a consistent grasp pose above the cube, execute it while keeping the gripper open, then move down and close the gripper to pick up the cube.    \n",
    "- Finally, open the gripper to release the cube into the bin.\n",
    "\n",
    "The planner may fail if the pose is unreachable, collides with the environment, or the solver does not converge. This is expectedâ€”adjust poses or retry if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "EPISODES = 10\n",
    "\n",
    "env = TrajEnv(\n",
    "    xml_file=\"PUT ABSOLUTE PATH HERE\", \n",
    "    frame_skip=20,\n",
    "    render_mode = \"rgb_array\"\n",
    ")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    \"videos\",                       # folder to save videos\n",
    "    episode_trigger=lambda e: True  # record every episode\n",
    "    )\n",
    "\n",
    "def pick_cube_solution(env, episode_data=None):\n",
    "    # TODO Use the planner to pick up the cube and drop it in the bin.\n",
    "\n",
    "    result = planner.plan_screw(\n",
    "        target_pose=???,           # target hand pose above cube (7,)\n",
    "        qpos=???,                  # current robot joint positions (9,)\n",
    "        time_step=env.dt,\n",
    "    )\n",
    "\n",
    "    # Follow the generated plan while keeping the gripper open (1)\n",
    "    if 'Success' in result:\n",
    "        for pos in result[\"position\"]:\n",
    "            env.step(np.concatenate([pos, [255]]))\n",
    "    else:\n",
    "        print(f\"Planner failed: {result}\")\n",
    "        return False  # Signal failure\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "successes = 0\n",
    "for i in range(EPISODES):\n",
    "    env.reset(seed=i)\n",
    "    pick_cube_solution(env)\n",
    "\n",
    "    success = env.terminated\n",
    "    successes += success\n",
    "env.close()\n",
    "print(f\"Success rate: {successes/EPISODES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collect your Imitation Learning Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that youâ€™ve used the motion planner to complete a single pick-and-place sequence, the next step is to repeat this process many times to collect a dataset for imitation learning.\n",
    "\n",
    "In imitation learning, the dataset is made of trajectoriesâ€”sequences of state-action pairs showing how an expert (in this case, the motion planner) solves the task. Each trajectory will later be used to train a policy that imitates this behavior.\n",
    "\n",
    ">**Tip**: Standardizing the orientation and approach of the gripper will help the robot pick up the cube in the same way each time, resulting in a more reliable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "env = TrajEnv(\n",
    "    xml_file=\"PUT ABSOLUTE PATH HERE\", \n",
    "    frame_skip=20,\n",
    ")\n",
    "\n",
    "trajectories = []\n",
    "\n",
    "EPISODES = 10_000\n",
    "success_count = 0\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    env.reset()\n",
    "    episode_data = []\n",
    "\n",
    "    # this sets the planner object up such that you can plan with poses in the world frame.\n",
    "    planner.set_base_pose(np.concatenate([env.data.xpos[env.panda_id], env.data.xquat[env.panda_id]]))\n",
    "\n",
    "    result = pick_cube_solution(env, episode_data)\n",
    "\n",
    "\n",
    "    if result:\n",
    "        trajectories.append(episode_data)\n",
    "        print(f\"Episode {i} completed successfully.\")\n",
    "        success_count += 1\n",
    "    \n",
    "    \n",
    "# ===== Save dataset =====\n",
    "with open(\"pick_place_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trajectories, f)\n",
    "\n",
    "print(f\"Success rate: {success_count / EPISODES}\")\n",
    "print(f\"Total trajectories: {len(trajectories)}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Behavior Cloning Dataset\n",
    "\n",
    "Using the trajectory data you have collected in the previous task, complete the `TrajectoryDataset` class to feed your model. \n",
    "\n",
    ">**NOTE**: The model learns to map states to actions, so the order of the data is not important. You can flatten all episodes and sample from any step freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #This should return observation idx and action idx\n",
    "        pass\n",
    "\n",
    "\n",
    "dataset_file = \"pick_place_dataset.pkl\"\n",
    "with open(dataset_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "dataset = TrajectoryDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define the Policy Network\n",
    "\n",
    "In this step, you will implement a behavior cloning policy network. The goal of this network is to learn to map observations (states) to actions by imitating the trajectories you collected earlier. \n",
    "\n",
    "**NOTE**: See [tips](tips.md) for guidance on designing your network architecture and BC overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #TODO: Define the network architecture\n",
    "        # Start with a basic MLP and experiment with different architectures\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Loop\n",
    "You will train the neural network policy to imitate expert behavior.\n",
    "\n",
    "Weâ€™ve provided an `eval_policy` function to test your actor in simulation. It runs several episodes and reports a success rate, so you can track how well your policy performs as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def eval_policy(eval_env: gym.Env, actor: nn.Module, num_episodes: int = 5):\n",
    "    success = 0\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = eval_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                action, _ = actor(torch.from_numpy(obs).float())\n",
    "            obs, _, terminated, truncated, _ = eval_env.step(action.cpu().numpy())\n",
    "\n",
    "        if terminated:\n",
    "            success += 1\n",
    "        \n",
    "        print(f\"Episode {ep+1}: Success={terminated}\")\n",
    "    return success / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "actor = Actor().to(device)\n",
    "eval_env = TrajEnv(\"PUT ABSOLUTE PATH HERE\", 20)\n",
    "\n",
    "batch_size = 1024\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "lr = 3e-5\n",
    "epochs = 100\n",
    "\n",
    "def nll_loss(actor: nn.Module, obs: torch.Tensor, expert_actions: torch.Tensor):\n",
    "    mean, log_std = actor(obs)\n",
    "    std = log_std.exp()\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    log_prob = dist.log_prob(expert_actions).sum(-1)   #sum over action dims\n",
    "    loss = -log_prob.mean()\n",
    "    return loss\n",
    "\n",
    "# TODO: Write the training loop\n",
    "# Train the actor policy to predict the actions in the dataset.\n",
    "# Make sure to run the eval_policy function every once in a while to track training progress\n",
    "# and save your best policy checkpoint to be loaded later for evaluation. \n",
    "# If you are confident in your model, sometimes a different seed can work. \n",
    "# Training time depends on number of samples and number of epochs, but should not take more than 10 minutes on google colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate in Simulator\n",
    "Time to evaluate your model. You may notice that it doesnâ€™t succeed very often and may behave unpredictably or erratically. \n",
    "\n",
    "Can you think about why this might happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "eval_env = TrajEnv(\"PUT ABSOLUTE PATH HERE\", 20, render_mode='rgb_array')\n",
    "eval_env = RecordVideo(\n",
    "    eval_env,\n",
    "    \"videos\",                       # folder to save videos\n",
    "    episode_trigger=lambda e: True  # record every episode\n",
    "    )\n",
    "actor = Actor().to(device)\n",
    "\n",
    "state_dict = torch.load('best_policy.pth')\n",
    "actor.load_state_dict(state_dict)\n",
    "\n",
    "#Evaluation\n",
    "eval_policy(eval_env, actor, 5)\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maniskill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
