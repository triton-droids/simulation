{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Code / Packages\n",
    "If you are running the code locally follow the instructions [here](../docs/setup.md).\n",
    "\n",
    "If you are using Google Colab make sure to run the two cells below to install all dependencies and enable headless rendering. You will also need to upload your xml file to the assets folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium==0.29.1 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from gymnasium[mujoco]==0.29.1) (0.29.1)\n",
      "Requirement already satisfied: numpy in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (1.13.1)\n",
      "Requirement already satisfied: matplotlib in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (3.9.4)\n",
      "Requirement already satisfied: imageio in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (2.37.0)\n",
      "Collecting imageio-ffmpeg\n",
      "  Downloading imageio_ffmpeg-0.6.0-py3-none-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from gymnasium==0.29.1->gymnasium[mujoco]==0.29.1) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from gymnasium==0.29.1->gymnasium[mujoco]==0.29.1) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from gymnasium==0.29.1->gymnasium[mujoco]==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: mujoco>=2.3.3 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from gymnasium[mujoco]==0.29.1) (3.3.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]==0.29.1) (2.3.1)\n",
      "Requirement already satisfied: etils[epath] in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]==0.29.1) (1.13.0)\n",
      "Requirement already satisfied: glfw in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]==0.29.1) (2.10.0)\n",
      "Requirement already satisfied: pyopengl in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]==0.29.1) (3.1.10)\n",
      "Requirement already satisfied: six>=1.5 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: fsspec in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]==0.29.1) (2025.9.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]==0.29.1) (6.5.2)\n",
      "Requirement already satisfied: zipp in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]==0.29.1) (3.23.0)\n",
      "Downloading imageio_ffmpeg-0.6.0-py3-none-macosx_11_0_arm64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0ma \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m:01\u001b[0mm\n",
      "Installing collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.2 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in /Users/darin/miniconda3/envs/mujoco_cpu/lib/python3.11/site-packages (from torchvision) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Core sim stack\n",
    "!pip -q install \"gymnasium[mujoco]==0.29.1\" numpy scipy matplotlib imageio imageio-ffmpeg\n",
    "\n",
    "# CPU-only PyTorch\n",
    "!pip -q install --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio\n",
    "\n",
    "%pip install \"gymnasium[mujoco]==0.29.1\" numpy scipy matplotlib imageio imageio-ffmpeg\n",
    "# Optional CPU-only torch wheel:\n",
    "%pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyvirtualdisplay'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyvirtualdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Display\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m Display(visible=\u001b[32m0\u001b[39m, size=(\u001b[32m1280\u001b[39m,\u001b[32m720\u001b[39m)).start()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyvirtualdisplay'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"] = \"glfw\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gymnasium\n",
    "No problems are assigned here, but understanding the code APIs/interfaces and functionalities are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gymnasium Basics\n",
    "The Gymnasium package is a standardized API for creating and interacting with simulation environments. \n",
    "\n",
    "<img src=\"../assets/media/gym_loop.png\" alt=\"agent-environment loop\" width=\"500\"/>\n",
    "\n",
    "Gymnasium defines the agent-environment loop — a simple interface where an agent takes actions and receives observations, rewards, and termination signals from an environment. This loop forms the foundation for reinforcement learning (RL) and allows us to easily manipulate the simulator.\n",
    "\n",
    "**Why We Use Gymnasium**\n",
    "\n",
    "Gymnasium acts as a common interface layer between learning algorithms and simulation environments. It provides:\n",
    "- A uniform API (reset(), step(), observation, reward, done, info) \n",
    "- Integration with RL algorithm libraries (e.g., Stable-Baselines3, CleanRL).\n",
    "- Rendering and debugging utilities for visualization and evaluation.\n",
    "\n",
    "\n",
    "In this task, you will use Gymnasium as the primary API to interface with the simulator to :\n",
    "- Collect trajectory data for imitation learning.\n",
    "- Evaluate and deploy trained policies.\n",
    "\n",
    "Read their documentation [here](https://gymnasium.farama.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Creating a Standardized Environment with Gymnasium\n",
    "In this part, you will implement the `TrajEnv` class to wrap a MuJoCo simulation in a Gymnasium interface.\n",
    "\n",
    "Your task is to decide which observations are useful as states in a behavior cloning setting. You will also implement the `is_grasped` property to define when the robot has successfully grasped the object and the `terminated` property to specify when the cube is in the bin. The `reset_model` method should randomize initial arm and object positions to encourage generalization.\n",
    "\n",
    "You should step through Gymnasium functions like `do_simulation` and `set_state` to see how to control the simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (394291649.py, line 16)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mlow=-np.inf, high=np.inf, shape=(???,), dtype=np.float32  # TODO: set correct shape\u001b[39m\n                                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "from gymnasium.spaces import Box\n",
    "import numpy as np\n",
    "import mujoco\n",
    "import os\n",
    "\n",
    "XML_PATH = os.path.abspath(\"assets/descriptions/DropCubeInBinEnv.xml\")\n",
    "\n",
    "class TrajEnv(MujocoEnv):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"depth_array\"]}\n",
    "\n",
    "    def __init__(self, xml_file: str, frame_skip: int = 5, **kwargs):\n",
    "        # TODO: Define the observation space and store any useful state variables.\n",
    "        observation_space = Box(\n",
    "            low=-np.inf, high=np.inf, shape=(???,), dtype=np.float32  # TODO: set correct shape\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            xml_file,\n",
    "            frame_skip,\n",
    "            observation_space=observation_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.panda_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_BODY, 'panda_arm')\n",
    "        \n",
    "\n",
    "    \n",
    "    @property\n",
    "    def is_grasped(self):\n",
    "        # TODO: Implement a condition to determine if the object\n",
    "        # is grasped. Could use distance between gripper and object, contact forces, etc.\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def terminated(self):\n",
    "        # TODO: Define the termination condition for an episode.\n",
    "        pass\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # TODO: Return the observations selected for the environment state\n",
    "        # Example: joint positions, velocities, end-effector pose, object positions\n",
    "        pass\n",
    "\n",
    "    def step(self, action: np.ndarray) -> tuple[np.ndarray, bool, bool]:\n",
    "        # DO NOT MODIFY THIS FUNCTION\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "        obs = self._get_obs()\n",
    "        terminated = self.terminated\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
    "        return obs, None, terminated, False, None\n",
    "\n",
    "    def reset_model(self):\n",
    "        self.default_qpos = np.array(\n",
    "                self.model.keyframe('home').qpos,\n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "        qpos = self.default_qpos.copy()\n",
    "        qvel = np.zeros(self.model.nv)\n",
    "\n",
    "        # TODO: Randomize initial positions for the arm (9 joints) and objects\n",
    "        # to create diverse starting conditions. Ensure valid states and objects are not overlapping.\n",
    "        \n",
    "\n",
    "        self.set_state(qpos, qvel)\n",
    "        return self._get_obs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what is happening in the environment, you can call `env.render()` to visualize it.  \n",
    "Since we set `render_mode=\"rgb_array\"`, `env.render()` will return an RGB array that can be displayed.  \n",
    "\n",
    "\n",
    "> **Note:** If the default view of the environment is not ideal, you may need to adjust the camera.  \n",
    "> You can do this by defining a camera in your Mujoco XML file and then passing its name as a kwarg to the `TrajEnv` constructor via the `camera_name` argument.\n",
    "\n",
    "\n",
    "If you are on colab you can render the environment like so. If you are running MuJoCo locally you can use `render_mode=\"human\"` to display viewing window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = TrajEnv(XML_PATH, 20, render_mode='rgb_array', camera_name=\"fixed\")\n",
    "env = TimeLimit(env, 100)\n",
    "env.reset()\n",
    "image = env.render() \n",
    "image = image\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these core functionalities we can build the basic interaction loop that consists of a reset followed by steps until terminated or truncated is True. We can also record a video as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "images = [env.render()]\n",
    "# keep taking steps until either we terminated or truncate\n",
    "while not done:\n",
    "    obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    images.append(env.render())\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import imageio\n",
    "\n",
    "frames = images \n",
    "frames = [frame.astype(np.uint8) for frame in frames]   \n",
    "\n",
    "# Write video to file\n",
    "video_path = \"output_video.mp4\"\n",
    "with imageio.get_writer(video_path, fps=20) as writer:\n",
    "    for frame in frames:\n",
    "        writer.append_data(frame)\n",
    "\n",
    "# Display video in notebook\n",
    "Video(video_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try and control the robot to grasp the cube, pick it up, and drop it in the wooden bin. \n",
    "\n",
    "The following code sets up a motion planning library for you and saves some useful variables. \n",
    "- `planner`: The planner objects uses screw motion planning.\n",
    "- `panda_hand_tcp`: The panda hand tool center point \"link\", which gives the current pose of the link we are controlling via the motion planning library. This is the point in the middle between the two panda arm grippers.\n",
    "- `cube`: The cube object, which you can use to acccess its pose.\n",
    "- `bin`: The bin object.\n",
    "\n",
    "The motion planner setup will generate a sequence joint position actions for all the joints excluding the gripper. The gripper actions you can manually pick (can range from 0 to 255). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplib\n",
    "import os\n",
    "\n",
    "env = TrajEnv(\n",
    "    xml_file=XML_PATH, \n",
    "    frame_skip=20,\n",
    "    render_mode=\"rgb_array\"\n",
    "    \n",
    ")\n",
    "env.reset(seed=42)\n",
    "\n",
    "link_names = [\n",
    "    'panda_link0','panda_link1','panda_link2','panda_link3','panda_link4',\n",
    "    'panda_link5','panda_link6','panda_link7','panda_link8','panda_hand',\n",
    "    'panda_hand_tcp','panda_leftfinger','panda_rightfinger',\n",
    "    'panda_leftfinger_pad','panda_rightfinger_pad'\n",
    "]\n",
    "joint_names = [\n",
    "    'panda_joint1','panda_joint2','panda_joint3','panda_joint4',\n",
    "    'panda_joint5','panda_joint6','panda_joint7','panda_finger_joint1','panda_finger_joint2'\n",
    "]\n",
    "\n",
    "planner = mplib.Planner(\n",
    "    urdf=os.path.abspath(\"assets/descriptions/panda/urdf/panda.urdf\"),\n",
    "    srdf=os.path.abspath(\"assets/descriptions/panda/urdf/panda.srdf\"),\n",
    "    user_link_names=link_names,\n",
    "    user_joint_names=joint_names,\n",
    "    move_group=\"panda_hand_tcp\",\n",
    "    joint_vel_limits=np.ones(7) * 0.8,\n",
    "    joint_acc_limits=np.ones(7) * 0.8,\n",
    ")\n",
    "\n",
    "# this sets the planner object up such that you can plan with poses in the world frame, which is the default frame of all pose data\n",
    "# in our simulator\n",
    "planner.set_base_pose(np.concatenate([env.data.xpos[env.panda_id], env.data.xquat[env.panda_id]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes for this task**\n",
    "\n",
    "The goal is to use the motion planner to pick up the cube and drop it into the bin.\n",
    "\n",
    "- You will need to access object positions and orientations from the simulator (env.data.xpos, env.data.xquat, etc.).\n",
    "- The motion planner requires a target pose (7,) for the robot hand (position + quaternion) and the current joint configuration (qpos). Both are expressed in the world frame.\n",
    "- Plan a trajectory to a consistent grasp pose above the cube, execute it while keeping the gripper open, then move down and close the gripper to pick up the cube.    \n",
    "- Finally, open the gripper to release the cube into the bin.\n",
    "\n",
    "The planner may fail if the pose is unreachable, collides with the environment, or the solver does not converge. This is expected—adjust poses or retry if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "EPISODES = 10\n",
    "\n",
    "env = TrajEnv(\n",
    "    xml_file=XML_PATH, \n",
    "    frame_skip=20,\n",
    "    render_mode = \"rgb_array\"\n",
    ")\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    \"videos\",                       # folder to save videos\n",
    "    episode_trigger=lambda e: True,  # record every episode\n",
    "    name_prefix=\"planner\"\n",
    "    )\n",
    "\n",
    "def pick_cube_solution(env, episode_data=None):\n",
    "    # TODO Use the planner to pick up the cube and drop it in the bin.\n",
    "\n",
    "    result = planner.plan_screw(\n",
    "        target_pose=???,           # target hand pose above cube (7,)\n",
    "        qpos=???,                  # current robot joint positions (9,)\n",
    "        time_step=env.dt,\n",
    "    )\n",
    "\n",
    "    # Follow the generated plan while keeping the gripper open (1)\n",
    "    if result[\"status\"] == \"Success\":\n",
    "        for pos in result[\"position\"]:\n",
    "            env.step(np.concatenate([pos, [255]]))\n",
    "    else:\n",
    "        print(f\"Planner failed: {result}\")\n",
    "        return False  # Signal failure\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "successes = 0\n",
    "for i in range(EPISODES):\n",
    "    env.reset(seed=i)\n",
    "    pick_cube_solution(env) \n",
    "\n",
    "    success = env.terminated\n",
    "    successes += success\n",
    "env.close()\n",
    "print(f\"Success rate: {successes/EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/planner-episode-0.mp4\", embed=True, width=320) # Watch replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collect your Imitation Learning Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve used the motion planner to complete a single pick-and-place sequence, the next step is to repeat this process many times to collect a dataset for imitation learning.\n",
    "\n",
    "In imitation learning, the dataset is made of trajectories—sequences of state-action pairs showing how an expert (in this case, the motion planner) solves the task. Each trajectory will later be used to train a policy that imitates this behavior.\n",
    "\n",
    ">**Tip**: Standardizing the orientation and approach of the gripper will help the robot pick up the cube in the same way each time, resulting in a more reliable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "env = TrajEnv(\n",
    "    xml_file=XML_PATH, \n",
    "    frame_skip=20,\n",
    ")\n",
    "\n",
    "trajectories = []\n",
    "\n",
    "EPISODES = 10_000\n",
    "success_count = 0\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    env.reset()\n",
    "    episode_data = []\n",
    "\n",
    "    # this sets the planner object up such that you can plan with poses in the world frame.\n",
    "    planner.set_base_pose(np.concatenate([env.data.xpos[env.panda_id], env.data.xquat[env.panda_id]]))\n",
    "\n",
    "    result = pick_cube_solution(env, episode_data)\n",
    "\n",
    "\n",
    "    if result:\n",
    "        trajectories.append(episode_data)\n",
    "        print(f\"Episode {i} completed successfully.\")\n",
    "        success_count += 1\n",
    "    \n",
    "    \n",
    "# ===== Save dataset =====\n",
    "with open(\"pick_place_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trajectories, f)\n",
    "\n",
    "print(f\"Success rate: {success_count / EPISODES}\")\n",
    "print(f\"Total trajectories: {len(trajectories)}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Behavior Cloning Dataset\n",
    "\n",
    "Using the trajectory data you have collected in the previous task, complete the `TrajectoryDataset` class to feed your model. \n",
    "\n",
    ">**NOTE**: The model learns to map states to actions, so the order of the data is not important. You can flatten all episodes and sample from any step freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #This should return observation idx and action idx\n",
    "        pass\n",
    "\n",
    "\n",
    "dataset_file = \"pick_place_dataset.pkl\"\n",
    "with open(dataset_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "dataset = TrajectoryDataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define the Policy Network\n",
    "\n",
    "In this step, you will implement a behavior cloning policy network. The goal of this network is to learn to map observations (states) to actions by imitating the trajectories you collected earlier. \n",
    "\n",
    "**NOTE**: See [tips](tips.md) for guidance on designing your network architecture and BC overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #TODO: Define the network architecture\n",
    "        # Start with a basic MLP and experiment with different architectures\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Loop\n",
    "You will train the neural network policy to imitate expert behavior.\n",
    "\n",
    "We’ve provided an `eval_policy` function to test your actor in simulation. It runs several episodes and reports a success rate, so you can track how well your policy performs as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def eval_policy(eval_env: gym.Env, actor: nn.Module, num_episodes: int = 5):\n",
    "    success = 0\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = eval_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                action, _ = actor(torch.from_numpy(obs).float())\n",
    "            obs, _, terminated, truncated, _ = eval_env.step(action.cpu().numpy())\n",
    "\n",
    "        if terminated:\n",
    "            success += 1\n",
    "        \n",
    "        print(f\"Episode {ep+1}: Success={terminated}\")\n",
    "    return success / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "actor = Actor().to(device)\n",
    "eval_env = TrajEnv(XML_PATH, 20)\n",
    "\n",
    "batch_size = 1024\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "lr = 3e-5\n",
    "epochs = 100\n",
    "\n",
    "def nll_loss(actor: nn.Module, obs: torch.Tensor, expert_actions: torch.Tensor):\n",
    "    mean, log_std = actor(obs)\n",
    "    std = log_std.exp()\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    log_prob = dist.log_prob(expert_actions).sum(-1)   #sum over action dims\n",
    "    loss = -log_prob.mean()\n",
    "    return loss\n",
    "\n",
    "# TODO: Write the training loop\n",
    "# Train the actor policy to predict the actions in the dataset.\n",
    "# Make sure to run the eval_policy function every once in a while to track training progress\n",
    "# and save your best policy checkpoint to be loaded later for evaluation. \n",
    "# If you are confident in your model, sometimes a different seed can work. \n",
    "# Training time depends on number of samples and number of epochs, but should not take more than 10 minutes on google colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate in Simulator\n",
    "Time to evaluate your model. You may notice that it doesn’t succeed very often and may behave unpredictably or erratically. \n",
    "\n",
    "Can you think about why this might happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "eval_env = TrajEnv(XML_PATH, 20, render_mode='rgb_array')\n",
    "eval_env = RecordVideo(\n",
    "    eval_env,\n",
    "    \"videos\",                       # folder to save videos\n",
    "    episode_trigger=lambda e: True,  # record every episode\n",
    "    name_prefix=\"bc-model\"\n",
    "    )\n",
    "actor = Actor().to(device)\n",
    "\n",
    "state_dict = torch.load('best_policy.pth')\n",
    "actor.load_state_dict(state_dict)\n",
    "\n",
    "#Evaluation\n",
    "eval_policy(eval_env, actor, 5)\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/bc-model-episode-0.mp4\", embed=True, width=320) # Watch our replay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mujoco_cpu)",
   "language": "python",
   "name": "mujoco_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
