{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e190ca0a",
   "metadata": {},
   "source": [
    "\n",
    "# ManiSkill Mini‑Project — Pose Estimation (P1) & Imitation Learning (P2)\n",
    "\n",
    "This is a **student-facing** assignment notebook. Implement all code in regions marked:\n",
    "\n",
    "```\n",
    "# YOUR CODE HERE\n",
    "# CODE ENDS HERE\n",
    "```\n",
    "\n",
    "**Recommended platform:** Google Colab with GPU (or any CUDA-enabled machine). CPU-only will be slow.\n",
    "\n",
    "> You will see **Reading** callouts with suggested references for each section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a780f",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment Setup (Colab)\n",
    "\n",
    "If you're on **Google Colab**, run the cell below (uncomment the `pip` commands first). Re-run the runtime if prompted after install.\n",
    "\n",
    "**Reading:** Gymnasium vectorized envs overview; ManiSkill setup notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabced1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment when running on Colab.\n",
    "# !pip install --upgrade pip\n",
    "# !pip install mani_skill==3.* gymnasium torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install mplib transforms3d h5py tqdm imageio[ffmpeg]\n",
    "\n",
    "import torch, sys\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb910245",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Part 1 — RGB → Pose (PickCube)\n",
    "\n",
    "**Goal:** Collect RGB frames and cube poses from `PickCube-v1`, build a dataset, implement a small CNN regressor (position + quaternion), train it, and then use predictions in a simple plan‑execute routine.\n",
    "\n",
    "**Readings**\n",
    "- ManiSkill observation structure & `obs_mode` (how RGB & state are organized).  \n",
    "- Gymnasium vectorized envs (why batching speeds things up):   \n",
    "- (Background) Unit quaternions for rotation representations and why normalization matters: https://stackoverflow.com/questions/8919086/why-are-quaternions-used-for-rotations and https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n",
    "- Quickstart notes on task options: https://maniskill.readthedocs.io/en/latest/user_guide/getting_started/quickstart.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73c7b3",
   "metadata": {},
   "source": [
    "\n",
    "## 1.0) Smoke Test — Create Env & Inspect Observations\n",
    "\n",
    "Create a small vectorized `PickCube-v1` environment, reset once, and print the top-level keys in the observation as well as the base camera keys.\n",
    "\n",
    "**Deliverable:** print statements confirming the keys and close the env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mani_skill\n",
    "import gymnasium as gym\n",
    "\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "# 1) Create env with a small num_envs (e.g., 4)\n",
    "# 2) Reset once to get an observation dict\n",
    "# 3) Print: top-level obs keys and base camera subkeys\n",
    "# 4) Close the env\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951a3c3",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1) Data Collection\n",
    "\n",
    "Collect a dataset of **RGB frames** and **cube poses** from `PickCube-v1`.\n",
    "\n",
    "- Aim for ~**4096** samples total (use a **vectorized** env; adjust lower if memory-limited).\n",
    "- Per reset:\n",
    "  - read RGB from `obs[\"sensor_data\"][\"base_camera\"][\"rgb\"]` with shape `[B, H, W, 3]` (uint8),\n",
    "  - read cube pose from `env.unwrapped.cube.pose.raw_pose` with shape `[B, 7]` (xyz + quaternion).\n",
    "- Accumulate batches until you reach your target, then concatenate to:\n",
    "  - `images: torch.Tensor [N, H, W, 3]` (on CPU)\n",
    "  - `poses:  torch.Tensor [N, 7]` (on CPU)\n",
    "\n",
    "**Tip:** Accessing tensors from the simulator and moving them between devices can require careful cloning.\n",
    "\n",
    "**Reading**\n",
    "- ManiSkill observation & wrappers overview (how obs are structured): https://maniskill.readthedocs.io/en/latest/user_guide/concepts/observation.html.\n",
    "- Gymnasium vector envs API (batched `reset`/`step` semantics): https://gymnasium.farama.org/api/vector/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "num_envs = 128\n",
    "total_samples = 4096\n",
    "\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "# 1) Create a vectorized PickCube env with obs_mode=\"rgb\" and render_mode=\"rgb_array\"\n",
    "# 2) Loop: reset, extract batch RGB and cube raw pose, append CPU copies to buffers\n",
    "# 3) Stop when collected >= total_samples\n",
    "# 4) Concatenate buffers into `images` and `poses` tensors\n",
    "images = None\n",
    "poses  = None\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n",
    "\n",
    "# Basic checks\n",
    "assert images is not None and poses is not None, \"You must populate images and poses.\"\n",
    "assert images.shape[0] == poses.shape[0], \"Counts should match.\"\n",
    "print(\"images:\", images.shape, images.dtype)\n",
    "print(\"poses:\",  poses.shape,  poses.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de768d7",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2) Dataset Class\n",
    "\n",
    "Implement a PyTorch `Dataset` that returns `(img, pose)` where:\n",
    "- `img`: `[3, H, W]` in `[0,1]` (`float32`), and\n",
    "- `pose`: `[7]` (`float32`).\n",
    "\n",
    "**Reading**\n",
    "- PyTorch: Writing custom `Dataset` / `DataLoader` and transforms (normalization, channel order): https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html and https://docs.pytorch.org/tutorials/beginner/data_loading_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class RGBPoseEstimationDataset(Dataset):\n",
    "    def __init__(self, images, poses):\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # Store references to images and poses\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "\n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        return 0\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # 1) Fetch one HxWx3 uint8 image and one 7-dim pose\n",
    "        # 2) Convert image to [3,H,W] float32 in [0,1]\n",
    "        # 3) Convert pose to float32\n",
    "        # 4) Return (image_tensor, pose_tensor)\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Instantiate the dataset\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "dataset = None\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n",
    "print(\"Dataset length:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efd432",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3) Pose Estimator\n",
    "\n",
    "Design a small CNN that encodes the image to a feature vector, then predicts:\n",
    "- **position** (3) and\n",
    "- **orientation quaternion** (4), re‑normalized to **unit norm** before returning.\n",
    "\n",
    "**Reading**\n",
    "- Why unit quaternions for 3D orientation; consequences of not normalizing.\n",
    "- Split-head design for pose regression: https://arxiv.org/pdf/1505.07427\n",
    "- Multi-task learning & hard parameter sharing (shared trunk + per-task heads): https://www.ruder.io/multi-task/ and https://arxiv.org/pdf/1706.05098\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd498b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class PoseEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # Define: encoder + two heads (position, quaternion)\n",
    "        # Keep the network small and fast.\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # 1) Encode\n",
    "        # 2) Predict pos(3) and quat(4)\n",
    "        # 3) Renormalize quaternion to unit length\n",
    "        # 4) Concatenate [pos, quat] and return\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f62f8",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4) Train Pose Regressor\n",
    "\n",
    "- Use `DataLoader` to batch your dataset.\n",
    "- Loss: **MSE** for positions and **MSE** for quaternions; combine via a weighted sum.\n",
    "- Track and plot training loss over epochs.\n",
    "- Save your best checkpoint if you like.\n",
    "\n",
    "**Reading**\n",
    "- PyTorch `DataLoader` basics (batching, shuffling): https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html and https://docs.pytorch.org/docs/stable/data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa553b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PoseEstimator().to(device)\n",
    "\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "# 1) Create DataLoader\n",
    "# 2) Define optimizer & regression losses\n",
    "# 3) Training loop: forward, compute weighted losses, backward, step\n",
    "# 4) Track `training_losses` for plotting\n",
    "training_losses = []\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_losses)\n",
    "plt.title(\"Pose Training Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903697c",
   "metadata": {},
   "source": [
    "\n",
    "## 1.5) Written — Representation & Transforms (P1.1)\n",
    "\n",
    "In 4–8 sentences, explain your image and pose representations and any transforms you apply. Justify each design choice (e.g., scaling, channel order, quaternion normalization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4e8a17",
   "metadata": {},
   "source": [
    "\n",
    "## 1.6) Plan & Execute with Predicted Pose\n",
    "\n",
    "Use your trained pose model to pick up the cube and move toward the goal bin.\n",
    "\n",
    "We provide environment setup and a `Planner`. Implement a function that:\n",
    "1) builds a normalized image batch from the observation,  \n",
    "2) predicts `[x,y,z,qx,qy,qz,qw]`,  \n",
    "3) computes a feasible end‑effector orientation, and  \n",
    "4) executes a simple approach → grasp → move routine.\n",
    "\n",
    "**Reading**\n",
    "- ManiSkill recording wrapper for videos: https://maniskill.readthedocs.io/en/latest/user_guide/wrappers/record.html\n",
    "- (Background) SciPy `Rotation` utilities for quaternion math: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup (provided; do not modify) ===\n",
    "import mani_skill.envs\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "import mplib\n",
    "from transforms3d.euler import euler2quat, quat2euler\n",
    "\n",
    "def create_env():\n",
    "    env = gym.make(\"PickCube-v1\", num_envs=1, obs_mode=\"rgb\",\n",
    "                   control_mode=\"pd_joint_pos\", render_mode=\"rgb_array\",\n",
    "                   reward_mode=\"none\", human_render_camera_configs=dict(shader_pack=\"default\"))\n",
    "    env = RecordEpisode(env, output_dir=\"pick_cube_mp\", video_fps=20, info_on_video=False, save_trajectory=False)\n",
    "    env.reset(seed=42)\n",
    "    return env\n",
    "\n",
    "env = create_env()\n",
    "robot = env.unwrapped.agent.robot\n",
    "link_names  = [link.get_name() for link in robot.get_links()]\n",
    "joint_names = [joint.get_name() for joint in robot.get_active_joints()]\n",
    "planner = mplib.Planner(\n",
    "    urdf=env.unwrapped.agent.urdf_path,\n",
    "    srdf=env.unwrapped.agent.urdf_path.replace(\".urdf\", \".srdf\"),\n",
    "    user_link_names=link_names,\n",
    "    user_joint_names=joint_names,\n",
    "    move_group=\"panda_hand_tcp\",\n",
    "    joint_vel_limits=np.ones(7)*0.8,\n",
    "    joint_acc_limits=np.ones(7)*0.8,\n",
    ")\n",
    "planner.set_base_pose(np.concatenate([robot.pose.sp.p, robot.pose.sp.q]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import torch, numpy as np\n",
    "\n",
    "def pick_cube_mp_solution(env, obs, goal_pos, model, device=None):\n",
    "    # YOUR CODE HERE -----------------------------------------------------------\n",
    "    # Implement a minimal routine:\n",
    "    # - Build input batch from observation\n",
    "    # - Predict pose\n",
    "    # - Convert quaternions to a target EE orientation\n",
    "    # - Plan & execute a short sequence with the planner and gripper control\n",
    "    # Return anything useful if desired.\n",
    "    # CODE ENDS HERE -----------------------------------------------------------\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Evaluation scaffold ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PoseEstimator().to(device)\n",
    "# Optionally: load your trained weights\n",
    "# model.load_state_dict(torch.load(\"best_pose_model.pth\", map_location=device))\n",
    "\n",
    "EPISODES = 3\n",
    "successes = 0\n",
    "for i in range(EPISODES):\n",
    "    obs, _ = env.reset(seed=i)\n",
    "    goal_pos = obs[\"extra\"][\"goal_pos\"].cpu().numpy()[0]\n",
    "    try:\n",
    "        pick_cube_mp_solution(env, obs, goal_pos, model, device=device)\n",
    "    except NotImplementedError:\n",
    "        print(\"Implement pick_cube_mp_solution first.\")\n",
    "        break\n",
    "    successes += env.get_info()[\"success\"].item()\n",
    "\n",
    "print(\"Success rate:\", successes/max(1, EPISODES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704ca4d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Part 2 — Imitation Learning (PushCube)\n",
    "\n",
    "You will:\n",
    "1) Inspect a demonstration dataset (`.h5` + `.json`),  \n",
    "2) Build a `TrajectoryDataset` mapping **state → action**,  \n",
    "3) Implement an `Actor` MLP and train via **behavior cloning**,  \n",
    "4) Evaluate the policy in the environment (don’t rely on training loss).\n",
    "\n",
    "**Reading**\n",
    "- (Basics) What behavior cloning is and how it relates to supervised learning: https://rail.eecs.berkeley.edu/deeprlcourse/deeprlcourse/static/slides/lec-2.pdf\n",
    "- Also check this one out from CMU: https://www.andrew.cmu.edu/course/10-403/slides/S19_lecture2_behaviorcloning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20161a4",
   "metadata": {},
   "source": [
    "\n",
    "## 2.0) Load & Inspect Demo File\n",
    "\n",
    "Load the `.h5` file and matching `.json` metadata. Print field names and shapes for one trajectory to understand the data layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py, json\n",
    "from mani_skill.utils.io_utils import load_json\n",
    "\n",
    "dataset_file = \"demos/PushCube-v1/motionplanning/trajectory.state.pd_joint_delta_pos.physx_cpu.h5\"\n",
    "data = h5py.File(dataset_file, \"r\")\n",
    "json_path = dataset_file.replace(\".h5\", \".json\")\n",
    "json_data = load_json(json_path)\n",
    "\n",
    "def load_h5_data(group):\n",
    "    out = {}\n",
    "    for k in group.keys():\n",
    "        if isinstance(group[k], h5py.Dataset):\n",
    "            out[k] = group[k][:]\n",
    "        else:\n",
    "            out[k] = load_h5_data(group[k])\n",
    "    return out\n",
    "\n",
    "# Preview one trajectory\n",
    "for k in data.keys():\n",
    "    print(\"Trajectory key:\", k)\n",
    "    traj = load_h5_data(data[k])\n",
    "    for kk, vv in traj.items():\n",
    "        if not isinstance(vv, dict):\n",
    "            print(\"-\", kk, getattr(vv, \"shape\", None))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3e1a1",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1) Eval Helpers (Provided)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "def create_eval_envs(json_data, num_envs=16, max_episode_steps=100, video_path=None):\n",
    "    env_info = json_data[\"env_info\"]\n",
    "    env_id = env_info[\"env_id\"]\n",
    "    env_kwargs = dict(env_info[\"env_kwargs\"])\n",
    "    env_kwargs.pop(\"num_envs\", None)\n",
    "    env_kwargs.pop(\"sim_backend\", None)\n",
    "    eval_envs = gym.make(env_id, num_envs=num_envs, reconfiguration_freq=1, max_episode_steps=max_episode_steps, **env_kwargs)\n",
    "    if video_path is not None:\n",
    "        eval_envs = RecordEpisode(eval_envs, output_dir=video_path, save_trajectory=False, max_steps_per_video=max_episode_steps)\n",
    "    eval_envs = ManiSkillVectorEnv(eval_envs, ignore_terminations=True, record_metrics=True)\n",
    "    return eval_envs\n",
    "\n",
    "def eval_policy(eval_envs, actor, verbose=1):\n",
    "    obs, _ = eval_envs.reset()\n",
    "    eval_metrics = defaultdict(list)\n",
    "    while True:\n",
    "        action = actor(obs)\n",
    "        obs, rew, terminated, truncated, info = eval_envs.step(action)\n",
    "        if truncated.any():\n",
    "            for k, v in info[\"final_info\"][\"episode\"].items():\n",
    "                eval_metrics[k].append(v.float())\n",
    "            break\n",
    "    if verbose:\n",
    "        for k in eval_metrics.keys():\n",
    "            print(f\"{k}_mean:\", torch.mean(torch.stack(eval_metrics[k])).item())\n",
    "    return eval_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ca8e7",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2) Build `TrajectoryDataset`\n",
    "\n",
    "Create a dataset that flattens all trajectories into pairs of `(obs_t, action_t)`.\n",
    "\n",
    "**Reading**\n",
    "- PyTorch custom `Dataset` patterns (index mapping across multiple sequences): https://docs.pytorch.org/tutorials/beginner/data_loading_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, data, json_data):\n",
    "        self.data = data\n",
    "        self.json_data = json_data\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # Build a flat index: list of (traj_key, t)\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "\n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        return 0\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # 1) Use index map to select a (traj_key, t)\n",
    "        # 2) Read observation vector and action vector at time t\n",
    "        # 3) Return dict with float32 tensors: {'obs': ..., 'action': ...}\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Instantiate\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "il_dataset = None\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n",
    "\n",
    "print(\"IL dataset length:\", len(il_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf8022",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3) Actor Network (MLP)\n",
    "\n",
    "Implement a small MLP mapping observation vectors → action vectors.\n",
    "\n",
    "**Reading**\n",
    "- Behavior cloning (conceptually supervised learning on state→action).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # Define a compact MLP\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE -------------------------------------------------------\n",
    "        # CODE ENDS HERE -------------------------------------------------------\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b29510",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4) Sanity Check Shapes\n",
    "\n",
    "Create eval envs, sample one batch, and verify the actor returns a tensor with the correct action shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa346554",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eval_envs = create_eval_envs(json_data, num_envs=16)\n",
    "sample_obs, _ = eval_envs.reset()\n",
    "sample_act = eval_envs.action_space.sample()\n",
    "\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "# Instantiate actor with correct input/output dims and run a forward pass\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n",
    "\n",
    "eval_envs.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711e060",
   "metadata": {},
   "source": [
    "\n",
    "## 2.5) Train Behavior Cloning (BC)\n",
    "\n",
    "Train an `Actor` by minimizing MSE between predicted and expert actions. Evaluate periodically by **running the policy in the environment**, not just by loss.\n",
    "\n",
    "**Things to think about**\n",
    "- Why evaluation in‑env is necessary vs. training loss alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f237792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eval_envs = create_eval_envs(json_data, num_envs=16, max_episode_steps=100)\n",
    "sample_obs, _ = eval_envs.reset(seed=0)\n",
    "sample_act = eval_envs.action_space.sample()\n",
    "\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "# 1) Instantiate Actor and move to device\n",
    "# 2) Create DataLoader over il_dataset\n",
    "# 3) Train for several epochs; periodically call eval_policy and track success rate\n",
    "# 4) Optionally save the best model\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n",
    "\n",
    "eval_envs.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694093c1",
   "metadata": {},
   "source": [
    "\n",
    "## 2.6) Final Evaluation (with Video)\n",
    "\n",
    "Load your best checkpoint (optional) and evaluate several rollouts while recording to disk using the provided recording wrapper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "\n",
    "# YOUR CODE HERE ---------------------------------------------------------------\n",
    "# 1) Re-create eval envs with video_path, reset with a fixed seed\n",
    "# 2) Re-instantiate Actor and optionally load best weights\n",
    "# 3) Run several evals, compute mean success rate\n",
    "# 4) Display one saved video if present\n",
    "# CODE ENDS HERE ---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1837d46e",
   "metadata": {},
   "source": [
    "\n",
    "## 2.7) Written — Why not just training loss? (P2.2)\n",
    "\n",
    "In 1–2 sentences, explain why minimizing MSE on the dataset is **not** sufficient to evaluate policy performance in sequential decision-making settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38f297",
   "metadata": {},
   "source": [
    "\n",
    "## 2.8) Written — Sense–Plan–Act vs End‑to‑End IL (P2.3)\n",
    "\n",
    "Write 4–8 sentences comparing training difficulty and data requirements for **sense–plan–act** vs **end‑to‑end imitation learning**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
