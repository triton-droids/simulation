{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Code / Packages\n",
    "If you are running the code locally follow the instructions [here](../docs/setup.md).\n",
    "\n",
    "If you are using Google Colab make sure to run the two cells below to install all dependencies. You will also need to upload your xml scene to the assets folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mujoco\n",
    "!pip install mujoco_mjx\n",
    "!pip install brax\n",
    "\n",
    "#Download the assets folder\n",
    "!git init temp-repo\n",
    "%cd temp-repo\n",
    "!git remote add -f origin https://github.com/triton-droids/simulation.git\n",
    "!git config core.sparseCheckout true\n",
    "!echo \"assets/*\" >> .git/info/sparse-checkout\n",
    "!git pull origin onboarding\n",
    "%cd ..\n",
    "!mv temp-repo/assets ./assets\n",
    "!rm -rf temp-repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MuJoCo for Colab\n",
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'egl'  # enables headless rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing the Pick-and-Place MJX Environment\n",
    "\n",
    "In this part, you will implement the `PickAndPlace` environment using MuJoCo and MJX.\n",
    "\n",
    "- `_init_env`: Initialize important environment variables such as the robotâ€™s home pose, cube and bin locations, and body/joint IDs.\n",
    "- `reset`: Reset the simulation state with randomized initial positions for the robot arm, cube, and bin. \n",
    "- `step`: Advance the simulation forward given an action. Update environment state, check if the cube has been grasped, compute the reward, and determine if the episode has ended.\n",
    "- `_get_obs`: Define the observation space.\n",
    "- `check_grasp`: Determine whether the cube is grasped by the robot.\n",
    "- `check_success`: Define the success condition by checking if the cube is inside the bin boundaries.\n",
    "- `_compute_reward`: Design a shaped reward that encourages approaching the cube, grasping it, and successfully placing it into the bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax.envs.base import PipelineEnv, State\n",
    "from mujoco.mjx._src import support\n",
    "from brax.io import mjcf\n",
    "from jax import numpy as jp\n",
    "from typing import Any\n",
    "import mujoco\n",
    "import jax\n",
    "\n",
    "XML_PATH = os.path.abspath(\"assets/descriptions/DropCubeInBinEnv.xml\")\n",
    "\n",
    "class PickAndPlace(PipelineEnv):\n",
    "    def __init__(self, xml_path: str, cfg, **kwargs: Any):\n",
    "        #DO NOT MODIFY\n",
    "        mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "\n",
    "        sys = mjcf.load_model(\n",
    "            mj_model\n",
    "        )\n",
    "        sys = sys.tree_replace(\n",
    "            {\n",
    "                \"opt.timestep\": cfg.sim.timestep,\n",
    "                \"opt.solver\": cfg.sim.solver,\n",
    "                \"opt.iterations\": cfg.sim.iterations,\n",
    "                \"opt.ls_iterations\": cfg.sim.ls_iterations,\n",
    "            }\n",
    "        )\n",
    "        kwargs[\"n_frames\"] = cfg.action.n_frames\n",
    "        kwargs[\"backend\"] = \"mjx\"\n",
    "\n",
    "        super().__init__(sys, **kwargs)\n",
    "\n",
    "        self._init_env()\n",
    "\n",
    "    def _init_env(self) -> None:\n",
    "        self.init_q = jp.array(self.sys.mj_model.keyframe(\"home\").qpos)\n",
    "\n",
    "        self.nu = self.sys.nu\n",
    "        self.nq = self.sys.nq\n",
    "        self.nv = self.sys.nv\n",
    "\n",
    "        self.tcp_body_id = support.name2id(self.sys, mujoco.mjtObj.mjOBJ_BODY, \"panda_hand\")\n",
    "\n",
    "        #TODO: Store any useful environment variables\n",
    "        \n",
    "\n",
    "    def reset(self, rng: jp.ndarray) -> State:\n",
    "        qpos = self.init_q.copy()\n",
    "        qvel = jp.zeros(self.nv)\n",
    "\n",
    "        # TODO: Randomize the initial positions of the robot arm, cube, and bin.\n",
    "\n",
    "\n",
    "        pipeline_state = self.pipeline_init(qpos, qvel) \n",
    "\n",
    "        state_info = {\n",
    "            \"rng\": rng,\n",
    "            \"step\": 0,\n",
    "            \"done\": jp.array(False),\n",
    "        }\n",
    "\n",
    "        # TODO: Initialize the environment state information.\n",
    "\n",
    "        \n",
    "        return State(\n",
    "            pipeline_state, obs, reward, done, metrics, state_info\n",
    "        )\n",
    "\n",
    "    def step(self, state: State, action: jp.ndarray) -> State:\n",
    "\n",
    "        pipeline_state = self.pipeline_step(state.pipeline_state, action)\n",
    "\n",
    "        success = self.check_success(pipeline_state)\n",
    "\n",
    "        # TODO: Update the environment state after each step.\n",
    "\n",
    "        state.info[\"step\"] += 1\n",
    "        state.info[\"step\"] = jp.where(\n",
    "            success | (state.info[\"step\"] > 500), 0, state.info[\"step\"]\n",
    "        )\n",
    "\n",
    "\n",
    "        # Episode is done if task is successful or max steps exceeded\n",
    "        done = success | (state.info[\"step\"] > 500)\n",
    "        state.info[\"done\"] = done\n",
    "\n",
    "        return state.replace(\n",
    "            pipeline_state = pipeline_state,\n",
    "            obs = obs,\n",
    "            reward = reward,\n",
    "            done = done.astype(jp.float32),\n",
    "        )\n",
    "    \n",
    "    def _get_obs(\n",
    "        self,\n",
    "        pipeline_state: State,\n",
    "        info: dict[str, Any],\n",
    "    ):\n",
    "        # TODO: Construct the observations for the agent.\n",
    "\n",
    "        return {\n",
    "            \"state\": obs,\n",
    "            \"privileged_state\": obs\n",
    "        }\n",
    "\n",
    "    def check_success(self, pipeline_state: State):\n",
    "        # TODO: Define the success condition for an episode.\n",
    "        pass\n",
    "    \n",
    "    def check_grasp(self, pipeline_state: State):\n",
    "        # TODO: Implement a condition to determine if the object\n",
    "        # is grasped. Could use distance between gripper and object, contact forces, etc.\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _compute_reward(self, pipeline_state: State, info: dict[str, Any]):\n",
    "        # TODO: Implement the reward function.\n",
    "        # HINT: How can we break this up into subtasks ?\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Configs in the Codebase\n",
    "In our codebase, we use Hydra and dataclasses to manage configurations. This keeps experiment settings (like simulation parameters, observation sizes, or agent hyperparameters) separate from the actual environment and training code, making it easy to reproduce results or run variations of the same experiment.\n",
    "\n",
    "The `PickAndPlaceConfig` defines all the key parameters needed for this task:\n",
    "- **sim**: Physics engine settings such as timestep and solver iterations.\n",
    "- **action**: How actions are repeated in the environment.\n",
    "- **ppo_agent**: PPO hyperparameters.\n",
    "- **ppo_networks**: Neural network configuration. \n",
    "- **seed**: Random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple\n",
    "\n",
    "@dataclass\n",
    "class PickAndPlaceConfig:\n",
    "    \"\"\"Configuration class for the MJX environment.\"\"\"\n",
    "\n",
    "    @dataclass\n",
    "    class SimConfig:\n",
    "        timestep: float = 0.002\n",
    "        solver: int = 2\n",
    "        iterations: int = 5 \n",
    "        ls_iterations: int = 8\n",
    "\n",
    "    @dataclass\n",
    "    class ActionConfig:\n",
    "        n_frames: int = 5\n",
    "    \n",
    "    @dataclass\n",
    "    class PPOAgentConfig:\n",
    "        num_timesteps: int = 10_000_000\n",
    "        num_evals: int = 100\n",
    "        episode_length: int = 500\n",
    "        unroll_length: int = 50\n",
    "        num_minibatches: int = 32\n",
    "        num_updates_per_batch: int = 4\n",
    "        discounting: float = 0.97\n",
    "        learning_rate: float = 3e-4\n",
    "        entropy_cost: float = 0.02\n",
    "        clipping_epsilon: float = 0.2\n",
    "        num_envs: int = 1024\n",
    "        batch_size: int = 256\n",
    "        seed: int = 42\n",
    "        normalize_observations: bool = True\n",
    "        action_repeat: int = 1\n",
    "        max_grad_norm: float = 1.0\n",
    "        num_resets_per_eval: int = 1\n",
    "    \n",
    "    @dataclass\n",
    "    class PPONetworksConfig:\n",
    "        policy_hidden_layer_sizes: Tuple[int, ...] = (256, 256, 256)\n",
    "        value_hidden_layer_sizes: Tuple[int, ...] = (256, 256, 256)\n",
    "        policy_obs_key: str = \"state\"\n",
    "        value_obs_key: str = \"privileged_state\"\n",
    "    \n",
    "    sim: SimConfig = field(default_factory=SimConfig)\n",
    "    action: ActionConfig = field(default_factory=ActionConfig)\n",
    "    ppo_agent: PPOAgentConfig = field(default_factory=PPOAgentConfig)\n",
    "    ppo_networks: PPONetworksConfig = field(default_factory=PPONetworksConfig)\n",
    "    seed: int = PPOAgentConfig.seed\n",
    "\n",
    "config = PickAndPlaceConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our train and evaluation environment. We define a factory function for building PPO networks with the architecture and observation settings specified in the config.\n",
    "\n",
    "We set things up this way because we will use Braxâ€™s PPO implementation, which expects an environment and a networks factory to run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "import functools\n",
    "\n",
    "env = PickAndPlace(\n",
    "        xml_path=XML_PATH,\n",
    "        cfg = config,\n",
    "    )\n",
    "    \n",
    "eval_env = PickAndPlace(\n",
    "    xml_path=XML_PATH,\n",
    "    cfg = config,\n",
    ")\n",
    "\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "    policy_hidden_layer_sizes=config.ppo_networks.policy_hidden_layer_sizes,\n",
    "    value_hidden_layer_sizes=config.ppo_networks.value_hidden_layer_sizes,\n",
    "    policy_obs_key=config.ppo_networks.policy_obs_key,\n",
    "    value_obs_key=config.ppo_networks.value_obs_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function for saving parameters that runs every:\n",
    "\n",
    "`num_timesteps` / `num_evals`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from flax.training import orbax_utils\n",
    "from brax.io import model\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "def policy_params_fn(current_step: int, make_policy: Any, params: Any):\n",
    "    # save checkpoints\n",
    "    orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "    save_args = orbax_utils.save_args_from_target(params)\n",
    "    path = os.path.join(\"checkpoints\", f\"step_{current_step}\")\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
    "    policy_path = os.path.join(path, \"policy\")\n",
    "    model.save_params(policy_path, (params[0], params[1].policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we set up Weights & Biases to monitor training progress and log metrics in real time and pass the algorithm parameters to the train function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax.training.agents.ppo import train as ppo\n",
    "from dataclasses import asdict\n",
    "import functools\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "run_name = \"PickandPlace\"\n",
    "wandb.init(\n",
    "    project=\"OnBoarding\",\n",
    "    name=run_name,\n",
    "    config=config.ppo_agent\n",
    "    )\n",
    "\n",
    "\n",
    "train_fn = functools.partial(\n",
    "        ppo.train,\n",
    "        **asdict(config.ppo_agent)\n",
    "    )\n",
    "\n",
    "times = [time.time()]\n",
    "\n",
    "last_ckpt_step = 0\n",
    "best_ckpt_step = 0\n",
    "best_episode_reward = -float(\"inf\")\n",
    "last_video_step = 0\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    global best_episode_reward, best_ckpt_step, last_ckpt_step, last_video_step\n",
    "\n",
    "    times.append(time.time())\n",
    "    wandb.log(metrics, step=num_steps)\n",
    "\n",
    "    last_ckpt_step = num_steps\n",
    "\n",
    "    episode_reward = float(metrics.get(\"eval/episode_reward\", 0))\n",
    "    if episode_reward > best_episode_reward:\n",
    "        best_episode_reward = episode_reward\n",
    "        best_ckpt_step = num_steps\n",
    "    print(f\"{num_steps}: {metrics['eval/episode_reward']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_inference_fn, params, _ = train_fn(\n",
    "    environment=env, eval_env=eval_env, progress_fn=progress\n",
    ")\n",
    "\n",
    "print(f\"time to jit: {times[1] - times[0]}\")\n",
    "print(f\"time to train: {times[-1] - times[1]}\")\n",
    "print(f\"best checkpoint step: {best_ckpt_step}\")\n",
    "print(f\"best episode reward: {best_episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapy as media\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rollout = []\n",
    "n_episodes = 1\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "  state = jit_reset(rng)\n",
    "  rollout.append(state)\n",
    "  for i in range(200):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state)\n",
    "\n",
    "render_every = 1\n",
    "frames = env.render(rollout[::render_every])\n",
    "rewards = [s.reward for s in rollout]\n",
    "media.show_video(frames, fps=1.0 / env.dt / render_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
